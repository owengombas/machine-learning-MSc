#+TITLE: Machine Learning and Data Mining
#+AUTHOR: Christos Dimitrakakis
#+EMAIL:christos.dimitrakakis@unine.ch
#+LaTeX_HEADER: \usepackage{tikz}
#+LaTeX_HEADER: \usepackage{amsmath}
#+LaTeX_HEADER: \usepackage{amssymb}
#+LaTeX_HEADER: \usepackage{isomath}
#+LaTeX_HEADER: \newcommand \E {\mathop{\mbox{\ensuremath{\mathbb{E}}}}\nolimits}
#+LaTeX_HEADER: \newcommand \Var {\mathop{\mbox{\ensuremath{\mathbb{V}}}}\nolimits}
#+LaTeX_HEADER: \newcommand \Bias {\mathop{\mbox{\ensuremath{\mathbb{B}}}}\nolimits}
#+LaTeX_HEADER: \newcommand\ind[1]{\mathop{\mbox{\ensuremath{\mathbb{I}}}}\left\{#1\right\}}
#+LaTeX_HEADER: \renewcommand \Pr {\mathop{\mbox{\ensuremath{\mathbb{P}}}}\nolimits}
#+LaTeX_HEADER: \DeclareMathOperator*{\argmax}{arg\,max}
#+LaTeX_HEADER: \DeclareMathOperator*{\argmin}{arg\,min}
#+LaTeX_HEADER: \DeclareMathOperator*{\sgn}{sgn}
#+LaTeX_HEADER: \newcommand \defn {\mathrel{\triangleq}}
#+LaTeX_HEADER: \newcommand \Reals {\mathbb{R}}
#+LaTeX_HEADER: \newcommand \Param {\Theta}
#+LaTeX_HEADER: \newcommand \param {\theta}
#+LaTeX_HEADER: \newcommand \vparam {\vectorsym{\theta}}
#+LaTeX_HEADER: \newcommand \mparam {\matrixsym{\Theta}}
#+LaTeX_HEADER: \newcommand \bW {\matrixsym{W}}
#+LaTeX_HEADER: \newcommand \bw {\vectorsym{w}}
#+LaTeX_HEADER: \newcommand \wi {\vectorsym{w}_i}
#+LaTeX_HEADER: \newcommand \wij {w_{i,j}}
#+LaTeX_HEADER: \newcommand \bA {\matrixsym{A}}
#+LaTeX_HEADER: \newcommand \ai {\vectorsym{a}_i}
#+LaTeX_HEADER: \newcommand \aij {a_{i,j}}
#+LaTeX_HEADER: \newcommand \bx {\vectorsym{x}}
#+LaTeX_HEADER: \newcommand \bel {\beta}
#+LaTeX_HEADER: \newcommand \Ber {\textrm{Bernoulli}}
#+LaTeX_HEADER: \newcommand \Beta {\textrm{Beta}}
#+LaTeX_HEADER: \newcommand \Normal {\textrm{Normal}}
#+LaTeX_CLASS_OPTIONS: [smaller]
#+COLUMNS: %40ITEM %10BEAMER_env(Env) %9BEAMER_envargs(Env Args) %4BEAMER_col(Col) %10BEAMER_extra(Extra)
#+TAGS: activity advanced definition exercise homework project example theory code
#+OPTIONS:   H:3
* General
** Introduction
*** Summary
This course gives an introduction to the algorithms and theory of
machine learning. Application is in the form of a course project.
During the course, you will be able to:

- Formulate machine learning problems in terms of opimisation or probabilistic inference.
- Understand the fundamental machine learning algorithms.
- Be able to implement some of the simplest algorithms.
- Apply off-the-shelf algorithms from scikit-learn to problems.
- Develop custom models using the pyTorch library.

The course focuses on algorithms and models, firstly on
optimisation-based learning, and the secondly on probabilistic
learning.


*** Schedule
|------+-------+----------------------------+----------------------------------+----------------------------+-----------------------------------------|
| Week |  Date | Topic                      | Theory                           | Lab                        | Assignment                              |
|------+-------+----------------------------+----------------------------------+----------------------------+-----------------------------------------|
|    1 | 09.26 | Supervisd Learning, kNN    | Decision theory                  | None                       |                                         |
|    2 | 10.03 | XV, Bootstrapping          | Generalisation                   | Train/Test/XV              | XV/Bootstrapping                        |
|    3 | 10.10 | Perceptron                 | SGD, Convergence                 | Perceptron, Classification | Perceptron (Logistic Regression) vs kNN |
|    4 | 10.17 | Discriminative models      | Linear Regression, Least-Squares | As Theory                  |                                         |
|    5 | 10.24 | Generative Models          | Bayes Classifier, GMM            | As Theory                  | GMM Bayes Classifier                    |
|    6 | 10.31 | Basis Functions            | GAMs                             | Feature selection          | Featire Selection                       |
|    7 | 11.07 | Multi-Layer Neural Network | Backpropagation                  | Network architectures      | Logistic Regression                     |
|    8 | 11.14 | Support Vector Machines    | Maximal Margin                   | SVMs                       |                                         |
|    9 | 11.21 | Regularisation             | Non-linear programming           | LASSO ?                    |                                         |
|   10 | 11.28 | Bayesian Inference         | Conjugate priors                 | Beta-Bernoulli             |                                         |
|   11 | 12.05 | Tree Models                | Expectation Maximisation         | Gaussian Mixture Model     |                                         |
|   12 | 12.12 | Sequential Moodels         | Markov Chain Monte Carlo         | MCMC                       |                                         |
|------+-------+----------------------------+----------------------------------+----------------------------+-----------------------------------------|
|   13 | 12.19 | Project Presentations      |                                  |                            |                                         |
|------+-------+----------------------------+----------------------------------+----------------------------+-----------------------------------------|

** Material
*** Textbooks
**** Primary
- Introduction to Statistical Learning with Python
https://hastie.su.domains/ISLP/ISLP_website.pdf
- Elements of Statistical Learning
https://hastie.su.domains/Papers/ESLII.pdf
**** Secondary
- Probabilistic Machine Learning: An Introduction
https://probml.github.io/pml-book/book1.html
https://github.com/probml/pml-book/releases/latest/download/book1.pdf
- Probabilistic Machine Learning: Advanced Topics
https://probml.github.io/pml-book/book2.html
https://github.com/probml/pml2-book/releases/latest/download/book2.pdf



**** Links to reference material

ISLP: Introduction to Statistical Learning with Python
ESL2: Elements of Statistical Learning (2nd Ed)
PML1: Probabilistic Machine Learning: An Introduction
PML2: Probabilistic Machine Learning: Advanced Topics

|-----------------------------+------+------+------+------|
| Topic                       | ISLP | ESL2 | PML1 | PML2 |
|-----------------------------+------+------+------+------|
| Linear Regression           |    3 |    3 |      |      |
| Nearest Neighbours          |  3,4 |   13 |      |      |
| Linear Classification       |    4 |    4 |      |      |
| Model Selection             |    5 |    7 |      |      |
| Linear Model Regularization |    6 |    3 |      |      |
| Basis Expansions            |    7 |    5 |      |      |
| Kernel Smoothing            |    7 |    6 |      |      |
| Additive Models             |    7 |    9 |      |      |
| Model Inference/Averaging   |    8 |    8 |      |      |
| Random Forests              |    8 |   15 |      |      |
| Ensemble Learning           |    8 |   16 |      |      |
| Trees                       |    8 |    9 |      |      |
| Boosting                    |    8 |   10 |      |      |
| Expectation Maximisation    |    * |    8 |      |  6.5 |
| SVMs                        |    9 |   12 |      |      |
| Neural Netowrks             |   10 |   11 |      |      |
| Censored Data               |   11 |   18 |      |      |
| Unsupervised Learning       |   12 |   14 |      |      |
| Undirected Graphical Models |    * |   17 |      |      |
| Hypothesis tesing           |   13 |   18 |      |      |
| High-Dimensional Statisitcs |    6 |   18 |      |      |
|-----------------------------+------+------+------+------|

