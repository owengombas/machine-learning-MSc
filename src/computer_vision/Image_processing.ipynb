{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Computer vision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/owen/miniconda/lib/python3.10/site-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "h , w = 28, 28\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "X, y = mnist['data'], mnist['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot image\n",
    "\n",
    "Get a random image from the dataset and plot it using matplotlib.\n",
    "Remember that after selecting an image you have to transform it into a matrix if (28,28) pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_image = 7993\n",
    "image_data = X.iloc[random_image].values # this is a vector we want matrix\n",
    "y_example = y.iloc[random_image]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_data.reshape((28,28)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = image_data.reshape((28,28)) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.imshow(image, cmap = \"Greys\")\n",
    "plt.title(f\"Example Digit, label = {y_example}\")\n",
    "plt.xlabel(\"Width\")\n",
    "plt.ylabel(\"Height\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalise data \n",
    "\n",
    "Normalize your data in order to scale the values from [0,1]\n",
    "In order to do so think of the maximum and the minimum value of each pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm = X/255 # convert image to value [0, 1]\n",
    "y = y.astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_norm.iloc[1234].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into train val test\n",
    "\n",
    "Split the dataset into training validation and test set using sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# create train test set\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(X_norm,\n",
    "                                                y,\n",
    "                                                test_size=0.2,\n",
    "                                                shuffle=True,\n",
    "                                                random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_dev,\n",
    "                                                  y_dev,\n",
    "                                                  test_size=0.1,\n",
    "                                                  shuffle=True,\n",
    "                                                  random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size:  (50400, 784)\n",
      "val set size:  (5600, 784)\n",
      "test set size:  (14000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(\"train set size: \", X_train.shape)\n",
    "print(\"val set size: \", X_val.shape)\n",
    "print(\"test set size: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert image to pytorch dataset\n",
    "\n",
    "Fill the following function that get a pytorch data loader from a numpy arrays.\n",
    "You can check our previous pytorch lab to write the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_dataloader(X, y, batch_size, shuffle ):\n",
    "    # fill your code\n",
    "    # covert data into tensors\n",
    "    train_x_tensor = torch.tensor(X, dtype=torch.float)\n",
    "    train_y_tensor = torch.tensor(y, dtype=torch.int)\n",
    "\n",
    "    # use the dataset class wrapper for tensors\n",
    "    train_tensor_dataset = torch.utils.data.TensorDataset(train_x_tensor,\n",
    "                                                          train_y_tensor)# there are more coplex dataset\n",
    "                                                                                     # i.e images or text data\n",
    "\n",
    "    # pass the dataset class into Dataloader to batch and shuffle your data\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_tensor_dataset,\n",
    "                                                   batch_size=64,\n",
    "                                                   shuffle=True)\n",
    "    \n",
    "    return train_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now get the loader for the different data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = get_dataloader(X_train.values, y_train.values, batch_size = 16, shuffle=True)\n",
    "val_dataloader = get_dataloader(X_val.values, y_val.values, batch_size = y_val.shape[0], shuffle=False)\n",
    "test_dataloader = get_dataloader(X_test.values, y_test.values, batch_size = y_test.shape[0], shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax\n",
    "\n",
    "Implement the softmax function.\n",
    "The function in applied to a array $z$ and return a normalised array with values between $[0,1]$\n",
    "\n",
    "The values of each element is calculated according to the following formula:\n",
    "\n",
    "$$ \\sigma(z_i) = \\frac{e^{z_{i}}}{\\sum_{j=1}^K e^{z_{j}}} \\ \\ \\ for\\ i=1,2,\\dots,K $$\n",
    "\n",
    "The advantage of softmax is that is we sum the result of the function always sum to one.\n",
    "So is used in machine learning problem in order to convert a vector of arbitrary values into a vector of propabilities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    # fill your code\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33333333, 0.33333333, 0.33333333])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array1 = np.array([1,1,1])\n",
    "softmax(array1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.21194156, 0.21194156, 0.57611688])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array2 = np.array([0,0,1])\n",
    "softmax(array2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.92508923e-04, 2.42609079e-03, 1.79265209e-02, 9.78754879e-01])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array2 = np.array([1, 2, 4, 8])\n",
    "softmax(array2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Model\n",
    "\n",
    "Define a simple neural network, as our the example in introduction to pytorch.\n",
    "Use 100 units for the first layer and 50 for the second layer.  \n",
    "The output units must be the same as the number of classes (10).  \n",
    "We also have to use softmax to transform the output in range $[0, 1]$ in order to represent propabilities as explained before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Simple two layer neural network for regression\n",
    "    \"\"\"\n",
    "    def __init__(self, num_input_features):\n",
    "        super().__init__()\n",
    "        \n",
    "        # layer 1\n",
    "\n",
    "        \n",
    "        # layer  2\n",
    "\n",
    "        \n",
    "        # layer output layer\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_ff_model = NeuralNetwork(num_input_features = X_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Training  Step----------------------------\n",
    "def training_step(model, input_data, optimizer, loss_fn):\n",
    "    # reset gradients of the optimizer\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # unfold data\n",
    "    x_batch, y_batch = input_data\n",
    "    \n",
    "    # get predictions\n",
    "    y_pred_propa = model(x_batch)\n",
    "    \n",
    "    # calculate loss\n",
    "    loss = loss_fn(y_pred_propa, y_batch)\n",
    "\n",
    "    # compute gradients \n",
    "    loss.backward()\n",
    "    \n",
    "    # optimise network\n",
    "    optimizer.step()\n",
    "    \n",
    "    # compute metrics for monitoring\n",
    "    with torch.no_grad(): \n",
    "        y_pred = torch.argmax(y_pred_propa,axis=1)\n",
    "        \n",
    "        train_acc = torch.sum(y_pred == y_batch) / y_batch.shape[0]\n",
    "\n",
    "    return loss.data.numpy(), train_acc.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_step(model, input_data, loss_fn):\n",
    "    # when we use torch.no_grad pytorch didnt store information\n",
    "    # that is required to calculate gradients so is fasterr \n",
    "    with torch.no_grad(): \n",
    "        x_batch, y_batch = input_data\n",
    "        y_pred_proba = model(x_batch)\n",
    "        loss = loss_fn(y_pred_proba, y_batch)\n",
    "\n",
    "        # compute metrics\n",
    "        y_pred = torch.argmax(y_pred_proba,axis=1)\n",
    "        acc = torch.sum(y_pred == y_batch) / y_batch.shape[0]\n",
    "    return loss.data.numpy(), acc.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "# -------------------- Train  Loop----------------------------\n",
    "def train_loop(train_dataloader, val_dataloader, patient, epochs, model, optimizer, loss_fn):\n",
    "    best_loss = np.inf\n",
    "    consecutive_epoch = 0\n",
    "    best_epoch = 0\n",
    "\n",
    "    train_history = []\n",
    "    val_history = []\n",
    "    for epoch in range(epochs): # iterate over epoch    \n",
    "\n",
    "        # -------------------- Training on each epoch ----------------------------\n",
    "        total_step = len(train_dataloader)\n",
    "        accumulated_loss = 0 # monitor loss during training\n",
    "        accumulated_accuracy = 0 # monitor  accuracy during training\n",
    "        accuracy_list = []\n",
    "        start = time.time()\n",
    "        for step in range(total_step): # iterate over batch\n",
    "            batch_data = next(iter(train_dataloader)) # get a batch\n",
    "            loss, accuracy = training_step(model,batch_data,optimizer,loss_fn) # train model using a single batch\n",
    "            accuracy_list += [accuracy]\n",
    "            accumulated_loss = (step * accumulated_loss + loss)/(step+1)\n",
    "            accumulated_accuracy =  (step * accumulated_accuracy + accuracy)/(step+1)\n",
    "        end = time.time()\n",
    "        total_time = end - start\n",
    "        train_history += [{\"loss\":accumulated_loss, \"accuracy\":accumulated_accuracy, \"epoch\": epoch, \"set\":\"train\"}]\n",
    "\n",
    "        # -------------------- Monitor Error Validation set ----------------------------\n",
    "        val_data = next(iter(val_dataloader))\n",
    "        val_loss, val_accuracy = evaluation_step(model, val_data, loss_fn)\n",
    "        val_history += [{\"loss\":val_loss, \"accuracy\":val_accuracy, \"epoch\": epoch, \"set\":\"val\"}]\n",
    "        if epoch % 1 == 0:\n",
    "            print(f\"Epoch {epoch}/{epochs}:({total_time:.3f} sec)  loss:{accumulated_loss:.3f}, accuracy-:{accumulated_accuracy:.3f}, val_loss:{val_loss:.3f}, val_accuracy->{val_accuracy:.3f}\")\n",
    "\n",
    "        # -------------------- Early Stoping ----------------------------\n",
    "        if val_loss > best_loss:\n",
    "            consecutive_epoch += 1\n",
    "        else:\n",
    "            best_loss = val_loss # we have an improvement\n",
    "            consecutive_epoch = 0 # reset counter\n",
    "            best_epoch = epoch\n",
    "            best_weights = model.state_dict()\n",
    "\n",
    "        if consecutive_epoch > patient:\n",
    "            break\n",
    "    val_history_df = pd.DataFrame(val_history)\n",
    "    train_history_df = pd.DataFrame(train_history)\n",
    "    return model, val_history_df, train_history_df, best_loss, best_epoch, best_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "optimizer got an empty parameter list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/owen/local/unibe/ml/machine-learning-MSc/src/computer_vision/Image_processing.ipynb Cell 35\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/owen/local/unibe/ml/machine-learning-MSc/src/computer_vision/Image_processing.ipynb#X46sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m lr \u001b[39m=\u001b[39m \u001b[39m0.001\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/owen/local/unibe/ml/machine-learning-MSc/src/computer_vision/Image_processing.ipynb#X46sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49moptim\u001b[39m.\u001b[39;49mAdam(simple_ff_model\u001b[39m.\u001b[39;49mparameters(), lr\u001b[39m=\u001b[39;49mlr)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/owen/local/unibe/ml/machine-learning-MSc/src/computer_vision/Image_processing.ipynb#X46sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m loss_fn \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mCrossEntropyLoss() \u001b[39m# cross entropy\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/owen/local/unibe/ml/machine-learning-MSc/src/computer_vision/Image_processing.ipynb#X46sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m out \u001b[39m=\u001b[39m train_loop(train_dataloader \u001b[39m=\u001b[39m train_dataloader,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/owen/local/unibe/ml/machine-learning-MSc/src/computer_vision/Image_processing.ipynb#X46sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                  val_dataloader \u001b[39m=\u001b[39m val_dataloader,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/owen/local/unibe/ml/machine-learning-MSc/src/computer_vision/Image_processing.ipynb#X46sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m                  patient \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/owen/local/unibe/ml/machine-learning-MSc/src/computer_vision/Image_processing.ipynb#X46sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m                  optimizer\u001b[39m=\u001b[39m optimizer,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/owen/local/unibe/ml/machine-learning-MSc/src/computer_vision/Image_processing.ipynb#X46sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m                  loss_fn\u001b[39m=\u001b[39m loss_fn)\n",
      "File \u001b[0;32m~/miniconda/lib/python3.10/site-packages/torch/optim/adam.py:45\u001b[0m, in \u001b[0;36mAdam.__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid weight_decay value: \u001b[39m\u001b[39m{\u001b[39;00mweight_decay\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m defaults \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(lr\u001b[39m=\u001b[39mlr, betas\u001b[39m=\u001b[39mbetas, eps\u001b[39m=\u001b[39meps,\n\u001b[1;32m     42\u001b[0m                 weight_decay\u001b[39m=\u001b[39mweight_decay, amsgrad\u001b[39m=\u001b[39mamsgrad,\n\u001b[1;32m     43\u001b[0m                 maximize\u001b[39m=\u001b[39mmaximize, foreach\u001b[39m=\u001b[39mforeach, capturable\u001b[39m=\u001b[39mcapturable,\n\u001b[1;32m     44\u001b[0m                 differentiable\u001b[39m=\u001b[39mdifferentiable, fused\u001b[39m=\u001b[39mfused)\n\u001b[0;32m---> 45\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(params, defaults)\n\u001b[1;32m     47\u001b[0m \u001b[39mif\u001b[39;00m fused:\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m differentiable:\n",
      "File \u001b[0;32m~/miniconda/lib/python3.10/site-packages/torch/optim/optimizer.py:261\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m    259\u001b[0m param_groups \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(params)\n\u001b[1;32m    260\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(param_groups) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 261\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39moptimizer got an empty parameter list\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    262\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(param_groups[\u001b[39m0\u001b[39m], \u001b[39mdict\u001b[39m):\n\u001b[1;32m    263\u001b[0m     param_groups \u001b[39m=\u001b[39m [{\u001b[39m'\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m'\u001b[39m: param_groups}]\n",
      "\u001b[0;31mValueError\u001b[0m: optimizer got an empty parameter list"
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "\n",
    "optimizer = torch.optim.Adam(simple_ff_model.parameters(), lr=lr)\n",
    "loss_fn = torch.nn.CrossEntropyLoss() # cross entropy\n",
    "\n",
    "out = train_loop(train_dataloader = train_dataloader,\n",
    "                 val_dataloader = val_dataloader,\n",
    "                 patient = 3, \n",
    "                 epochs = 10,\n",
    "                 model = simple_ff_model,\n",
    "                 optimizer= optimizer,\n",
    "                 loss_fn= loss_fn)\n",
    "\n",
    "model, val_history_es_5, train_history_es_5, best_loss, best_epoch, best_weights = out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andreasathanasopoulos/Phd/projects/bayesian_fairness/envs/bayesian-fairness/lib/python3.9/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "test_data = next(iter(test_dataloader))\n",
    "loss, acc = evaluation_step(simple_ff_model, test_data, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(1.5008513, dtype=float32), array(0.96021426, dtype=float32))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cnn model\n",
    "\n",
    "Define a cnn architecture, with two cnn layers using 16 and 32 kernels respectively.\n",
    "Each kernel on both layers in has a size 5x5.\n",
    "After each layer we apply Relu activation function and max-pooling with kernel size 2 and stride 2.\n",
    "\n",
    "At the end you add a simple linear layer on the flatten representation of the final cnn architecture.\n",
    "\n",
    "Check the supplementary slide to calculate the correct input shape for you layers, otherwise you will get an error. (final slides)\n",
    "\n",
    "Fill the code bellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnNeuralNetwork(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Simple two layer neural network for regression\n",
    "    \"\"\"\n",
    "    def __init__(self, num_of_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        # layer 1\n",
    "        self.cnn_layer_1 = torch.nn.Conv2d(in_channels=,    # input channels           \n",
    "                                           out_channels=,   # number of kernels\n",
    "                                           kernel_size=  # kenrel size \n",
    "                                          )\n",
    "        self.activation_1 = # relu\n",
    "        self.polling_1 = torch.nn.MaxPool2d(kernel_size=) # maxpolling\n",
    "        \n",
    "        # layer  2\n",
    "        self.cnn_layer_2 = torch.nn.Conv2d(\n",
    "                                          in_channels=,  # input channels           \n",
    "                                          out_channels=, # number of kernels        \n",
    "                                          kernel_size= # kenrel size \n",
    "                                          )\n",
    "        self.activation_2 =  # relu\n",
    "        self.polling_2 = torch.nn.MaxPool2d() # maxpolling\n",
    "        \n",
    "        # layer output layer\n",
    "        self.out_layer = torch.nn.Linear(in_features=, # inputs that depends on the output of the cnn\n",
    "                                         out_features=)\n",
    "        self.out_activation= torch.nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # construct the forward pass\n",
    "        x_1 = self.polling_1(self.activation_1( self.cnn_layer_1( x ) ))\n",
    "        x_2 = self.polling_2(self.activation_2( self.cnn_layer_2( x_1 ) ))\n",
    "        \n",
    "        flaten = torch.flatten(x_2,start_dim=1)\n",
    "        \n",
    "        output = self.out_activation(self.out_layer(flaten))\n",
    "        return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = CnnNeuralNetwork(num_of_channels = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cnn_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcnn_model\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cnn_model' is not defined"
     ]
    }
   ],
   "source": [
    "cnn_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create data loader for the cnn model\n",
    "\n",
    "The cnn model takes as input a batch of images of (28,28) shape.\n",
    "In pytorch use the first dimension of the input as the channel information, so it receives input batches of shape: (batch_size, Channels, H, W)\n",
    "Call the get_dataloader with the right arguments in order to produce the input in the correct format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = get_dataloader(# add corect argument,\n",
    "                                  y_train.values,\n",
    "                                  batch_size = 16,\n",
    "                                  shuffle=True)\n",
    "\n",
    "val_dataloader = get_dataloader(# add corect argument,\n",
    "                                y_val.values,\n",
    "                                batch_size = y_val.shape[0],\n",
    "                                shuffle=False)\n",
    "\n",
    "test_dataloader = get_dataloader(# add corect argument,\n",
    "                                 y_test.values,\n",
    "                                 batch_size = y_test.shape[0],\n",
    "                                 shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/15:(13.262 sec)  loss:1.592, accuracy-:0.872, val_loss:1.492, val_accuracy->0.970\n",
      "Epoch 1/15:(13.120 sec)  loss:1.485, accuracy-:0.977, val_loss:1.488, val_accuracy->0.975\n",
      "Epoch 2/15:(13.136 sec)  loss:1.480, accuracy-:0.982, val_loss:1.492, val_accuracy->0.971\n",
      "Epoch 3/15:(12.994 sec)  loss:1.477, accuracy-:0.985, val_loss:1.481, val_accuracy->0.980\n",
      "Epoch 4/15:(12.980 sec)  loss:1.475, accuracy-:0.987, val_loss:1.479, val_accuracy->0.982\n",
      "Epoch 5/15:(13.429 sec)  loss:1.474, accuracy-:0.987, val_loss:1.478, val_accuracy->0.983\n",
      "Epoch 6/15:(13.001 sec)  loss:1.472, accuracy-:0.989, val_loss:1.480, val_accuracy->0.981\n",
      "Epoch 7/15:(13.318 sec)  loss:1.471, accuracy-:0.990, val_loss:1.478, val_accuracy->0.983\n",
      "Epoch 8/15:(13.117 sec)  loss:1.470, accuracy-:0.991, val_loss:1.474, val_accuracy->0.988\n",
      "Epoch 9/15:(12.985 sec)  loss:1.470, accuracy-:0.991, val_loss:1.479, val_accuracy->0.982\n",
      "Epoch 10/15:(13.033 sec)  loss:1.470, accuracy-:0.991, val_loss:1.475, val_accuracy->0.986\n",
      "Epoch 11/15:(12.998 sec)  loss:1.470, accuracy-:0.991, val_loss:1.472, val_accuracy->0.989\n",
      "Epoch 12/15:(12.981 sec)  loss:1.470, accuracy-:0.992, val_loss:1.475, val_accuracy->0.987\n",
      "Epoch 13/15:(13.086 sec)  loss:1.469, accuracy-:0.992, val_loss:1.474, val_accuracy->0.987\n",
      "Epoch 14/15:(13.007 sec)  loss:1.469, accuracy-:0.993, val_loss:1.475, val_accuracy->0.986\n"
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "\n",
    "optimizer = torch.optim.Adam(cnn_model.parameters(), lr=lr)\n",
    "loss_fn = torch.nn.CrossEntropyLoss() # binary cross entropy\n",
    "\n",
    "out = train_loop(train_dataloader = train_dataloader,\n",
    "                 val_dataloader = val_dataloader,\n",
    "                 patient = 3, \n",
    "                 epochs = 15,\n",
    "                 model = cnn_model,\n",
    "                 optimizer= optimizer,\n",
    "                 loss_fn= loss_fn)\n",
    "\n",
    "model, val_history_es_5, train_history_es_5, best_loss, best_epoch, best_weights = out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = next(iter(test_dataloader))\n",
    "loss, acc = evaluation_step(cnn_model, test_data, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mloss\u001b[49m, acc\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss' is not defined"
     ]
    }
   ],
   "source": [
    "loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
